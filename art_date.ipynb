{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Art Dating\n",
    "\n",
    "#### Students\n",
    "- Zhenbang Chen\n",
    "- Zhenjia Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Importing packages and dependencies.  Load dataset for categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "rootpath = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\zhenbang\\anaconda3\\lib\\site-packages (4.39.0)\n",
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# You might not have tqdm, which gives you nice progress bars\n",
    "!pip install tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, resume_from=None):\n",
    "    \n",
    "    if resume_from:\n",
    "        model_ft = models.resnet50(pretrained=False)\n",
    "        \n",
    "        in_features = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        model_ft.load_state_dict(torch.load(resume_from))\n",
    "        \n",
    "        return model_ft\n",
    "    else:\n",
    "        # Model (nn.Module) to return\n",
    "        # model_ft = models.resnet18(pretrained = false)\n",
    "        model_ft = models.resnet50(pretrained=False)\n",
    "\n",
    "        in_features = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to apply to the data\n",
    "# transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "# ])\n",
    "\n",
    "# Transform to apply to the data for use with pretrained ResNet model\n",
    "transform = torchvision.transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data from directory\n",
    "art_train = torchvision.datasets.ImageFolder(root=\"./data/art_culture_train\",\n",
    "                                                 transform=transform)\n",
    "\n",
    "# Get validation data from directory\n",
    "art_val = torchvision.datasets.ImageFolder(root=\"./data/art_culture_val\",\n",
    "                                               transform=transform)\n",
    "\n",
    "# Get testing data from directory\n",
    "art_test = torchvision.datasets.ImageFolder(root=\"./data/art_culture_test\",\n",
    "                                               transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random sampler\n",
    "random_sampler = torch.utils.data.RandomSampler(data_source=art_train,\n",
    "                                                replacement=True,\n",
    "                                                num_samples=int(len(art_train)/10))\n",
    "\n",
    "# Create batched dataloader\n",
    "art_train_loader = torch.utils.data.DataLoader(dataset=art_train,\n",
    "                                                   batch_size=8,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "# art_train_loader = torch.utils.data.DataLoader(dataset=art_train,\n",
    "#                                                    batch_size=8,\n",
    "#                                                    sampler=random_sampler,\n",
    "#                                                    shuffle=False,\n",
    "#                                                    num_workers=4,\n",
    "#                                                    pin_memory=True)\n",
    "\n",
    "art_val_loader = torch.utils.data.DataLoader(dataset=art_val,\n",
    "                                                 batch_size=8,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "art_test_loader = torch.utils.data.DataLoader(dataset=art_test,\n",
    "                                                 batch_size=8,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# model = torchvision.models.resnet18(pretrained=False)\n",
    "# model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Set number of output classes\n",
    "# model.conv1 = nn.Conv2d(in_channels=3,\n",
    "#                         out_channels=64,\n",
    "#                         kernel_size=(7,7),\n",
    "#                         stride=(2,2),\n",
    "#                         padding=(3,3),\n",
    "#                         bias=False)\n",
    "\n",
    "# in_features = model.fc.in_features\n",
    "# out_features = 11\n",
    "# model.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "model = initialize_model(num_classes=11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method\n",
    "def train(net, optim, criterion,train_loader):\n",
    "    net.train()\n",
    "    for image_cpu, label_cpu in tqdm(train_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Forward through the network\n",
    "        output = net(image)\n",
    "        \n",
    "        # Loss and gradient\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update paramters\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation method\n",
    "def evaluate(net, val_loader, top_n=1):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for image_cpu, label_cpu in tqdm(val_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        tiled_labels = torch.stack([label for _ in range(top_n)], dim=1) \n",
    "\n",
    "        \n",
    "        # Don't track gradients for performance in evaluation\n",
    "        with torch.no_grad():\n",
    "            # Get prediction with forward pass\n",
    "#             prediction = net(image).argmax(dim=-1)\n",
    "            \n",
    "            # Get the indices of the top_n predictions\n",
    "            prediction = net(image).topk(k=top_n, dim=-1)[1]\n",
    "            \n",
    "            # Total number in batch\n",
    "            total += image.size(0)\n",
    "            \n",
    "            # Number correct in batch\n",
    "#             correct += (prediction == label).sum().item()\n",
    "            # Number correct in batch\n",
    "            correct += (prediction == tiled_labels).sum().item()\n",
    "            \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation method\n",
    "def validate(net, optim, criterion, val_loader):\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for image_cpu, label_cpu in tqdm(val_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Don't track gradients for performance in validation\n",
    "        with torch.no_grad():\n",
    "            # Forward through the network\n",
    "            output = net(image)\n",
    "            \n",
    "            # Get prediction with forward pass\n",
    "            prediction = output.argmax(dim=-1)\n",
    "\n",
    "            # Loss and gradient\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            # Total number in batch\n",
    "            total += image.size(0)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Number correct in batch\n",
    "            correct += (prediction == label).sum().item()\n",
    "            \n",
    "    return running_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=4, gamma=0.1)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "#                                                  mode=\"min\",\n",
    "#                                                  factor=0.2,\n",
    "#                                                  patience=1,\n",
    "#                                                  verbose=True,\n",
    "#                                                  threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee7fee14afd4266b82b3e8ae7b4683d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=668), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "num_epochs = 15\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "best_state_dict = {}\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "#     val_acc = evaluate(model, art_val_loader) * 100\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    train(model, optimizer, criterion, art_train_loader)\n",
    "    val_loss, val_acc = validate(model, optimizer, criterion, art_val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(\"Val Loss - Epoch {}: {}\".format(epoch, val_loss))\n",
    "    print(\"Val Acc - Epoch {}: {}%\".format(epoch, val_acc))\n",
    "    \n",
    "    if epoch % 4 == 0 and epoch != 0:\n",
    "        torch.save(best_state_dict, \"./models/art_culture_temp_epoch\" + str(epoch))\n",
    "    \n",
    "print(\"Done! {}%\".format(evaluate(model, art_val_loader) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(best_state_dict, \"./models/art_culture_steplr0001_step3_gamma01_epoch12_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model to test\n",
    "model = initialize_model(num_classes=11,\n",
    "                         resume_from=\"./models/art_culture_platlr0001_fac02_pat1_thres001_epoch30\").to(device)\n",
    "\n",
    "test_acc = evaluate(net=model, val_loader=art_test_loader, top_n=3)\n",
    "print(\"Done!\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culture_categories = {\n",
    "    0 : \"american\",\n",
    "    1 : \"british\",\n",
    "    2 : \"chinese\",\n",
    "    3 : \"dutch\",\n",
    "    4 : \"flemish\",\n",
    "    5 : \"french\",\n",
    "    6 : \"german\",\n",
    "    7 : \"indian\",\n",
    "    8 : \"italian\",\n",
    "    9 : \"japanese\",\n",
    "    10 : \"spanish\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on one specific example\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image(image_name):\n",
    "    \"\"\"Loads an image; returns a batched image tensor\"\"\"\n",
    "    image = Image.open(image_name).convert(\"RGB\")\n",
    "#     plt.imshow(image)\n",
    "    \n",
    "    image = transform(image)\n",
    "    \n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    \n",
    "    image = image.clone().detach().requires_grad_(True)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def classify_image(model, categories, image_name):\n",
    "    '''Classify an image using a specified model'''\n",
    "    model.eval()\n",
    "    image = load_image(image_name).to(device)\n",
    "    result_vec = model(image)\n",
    "    \n",
    "    results = result_vec.sort(descending=True)[1][0].tolist()\n",
    "#     top_result = result_vec.argmax(dim=-1).item()\n",
    "    \n",
    "    return [(categories[index],) for index in results]\n",
    "\n",
    "model = initialize_model(num_classes=11,\n",
    "                         resume_from=\"./models/art_culture_platlr0001_fac02_pat1_thres001_epoch30\").to(device)\n",
    "\n",
    "culture = \"chinese\"\n",
    "image_index = \"103\"\n",
    "\n",
    "print(classify_image(model,\n",
    "                     culture_categories,\n",
    "                     \"test_images/grant_wood.jpg\"))\n",
    "\n",
    "# print(classify_image(model,\n",
    "#                      culture_categories,\n",
    "#                      \"./data/art_culture_all_filtered/\" + culture + \"/\" + str(image_index) + \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing data from directory\n",
    "letters_val = torchvision.datasets.ImageFolder(root=\"./data/text_val\",\n",
    "                                               transform=transform)\n",
    "\n",
    "letters_val_loader = torch.utils.data.DataLoader(dataset=letters_val,\n",
    "                                                 batch_size=512,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "# Initialize model\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "# model = torchvision.models.resnet50(pretrained=False)\n",
    "\n",
    "# Set number of output classes\n",
    "model.conv1 = nn.Conv2d(in_channels=3,\n",
    "                        out_channels=64,\n",
    "                        kernel_size=(7,7),\n",
    "                        stride=(2,2),\n",
    "                        padding=(3,3),\n",
    "                        bias=False)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "out_features = 26\n",
    "model.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"./models/letter_model_lr01_gamma015_e12\"))\n",
    "\n",
    "val_acc = evaluate(model, letters_val_loader) * 100\n",
    "print(\"Done!\", val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
