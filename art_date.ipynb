{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Art Dating\n",
    "\n",
    "#### Students\n",
    "- Zhenbang Chen\n",
    "- Zhenjia Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Importing packages and dependencies.  Load dataset for categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "rootpath = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\zhenbang\\anaconda3\\lib\\site-packages (4.39.0)\n",
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# You might not have tqdm, which gives you nice progress bars\n",
    "!pip install tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, resume_from=None):\n",
    "    \n",
    "    if resume_from:\n",
    "        model_ft = models.resnet50(pretrained=False)\n",
    "#         model_ft = models.vgg16_bn(pretrained=False)\n",
    "        \n",
    "        in_features = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(in_features, num_classes)\n",
    "#         in_features = model_ft.classifier[6].in_features\n",
    "#         model_ft.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        model_ft.load_state_dict(torch.load(resume_from))\n",
    "        \n",
    "        return model_ft\n",
    "    else:\n",
    "        # Model (nn.Module) to return\n",
    "        model_ft = models.resnet50(pretrained=True)\n",
    "#         model_ft = models.vgg16_bn(pretrained=True)\n",
    "\n",
    "        in_features = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(in_features, num_classes)\n",
    "#         in_features = model_ft.classifier[6].in_features\n",
    "#         model_ft.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "\n",
    "        return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to apply to the data\n",
    "# transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "# ])\n",
    "\n",
    "# Transform to apply to the data for use with pretrained ResNet model\n",
    "transform = torchvision.transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data from directory\n",
    "art_train = torchvision.datasets.ImageFolder(root=\"./data/art_culture_train\",\n",
    "                                                 transform=transform)\n",
    "\n",
    "# Get validation data from directory\n",
    "art_val = torchvision.datasets.ImageFolder(root=\"./data/art_culture_val\",\n",
    "                                               transform=transform)\n",
    "\n",
    "# Get testing data from directory\n",
    "art_test = torchvision.datasets.ImageFolder(root=\"./data/art_culture_test\",\n",
    "                                               transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random sampler\n",
    "random_sampler = torch.utils.data.RandomSampler(data_source=art_train,\n",
    "                                                replacement=True,\n",
    "                                                num_samples=int(len(art_train)/10))\n",
    "\n",
    "# Create batched dataloader\n",
    "art_train_loader = torch.utils.data.DataLoader(dataset=art_train,\n",
    "                                                   batch_size=8,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=4,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "# art_train_loader = torch.utils.data.DataLoader(dataset=art_train,\n",
    "#                                                    batch_size=8,\n",
    "#                                                    sampler=random_sampler,\n",
    "#                                                    shuffle=False,\n",
    "#                                                    num_workers=4,\n",
    "#                                                    pin_memory=True)\n",
    "\n",
    "art_val_loader = torch.utils.data.DataLoader(dataset=art_val,\n",
    "                                                 batch_size=8,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "art_test_loader = torch.utils.data.DataLoader(dataset=art_test,\n",
    "                                                 batch_size=8,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "# model = torchvision.models.resnet18(pretrained=False)\n",
    "# model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Set number of output classes\n",
    "# model.conv1 = nn.Conv2d(in_channels=3,\n",
    "#                         out_channels=64,\n",
    "#                         kernel_size=(7,7),\n",
    "#                         stride=(2,2),\n",
    "#                         padding=(3,3),\n",
    "#                         bias=False)\n",
    "\n",
    "# in_features = model.fc.in_features\n",
    "# out_features = 11\n",
    "# model.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "model = initialize_model(num_classes=11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify Network\n",
    "\n",
    "def transfer_learning():\n",
    "    in_features = model.fc.in_features\n",
    "    num_classes = model.fc.out_features\n",
    "    hidden_size = 512\n",
    "\n",
    "    dense_network = nn.Sequential(\n",
    "        nn.Linear(in_features, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, num_classes)\n",
    "    ).to(device)\n",
    "\n",
    "    model.fc = dense_network\n",
    "    \n",
    "# transfer_learning()\n",
    "    \n",
    "# for index, child in enumerate(model.children()):\n",
    "#     if index != 9:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method\n",
    "def train(net, optim, criterion,train_loader):\n",
    "    net.train()\n",
    "    for image_cpu, label_cpu in tqdm(train_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Forward through the network\n",
    "        output = net(image)\n",
    "        \n",
    "        # Loss and gradient\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update paramters\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation method\n",
    "def evaluate(net, val_loader, top_n=1):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for image_cpu, label_cpu in tqdm(val_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        tiled_labels = torch.stack([label for _ in range(top_n)], dim=1) \n",
    "\n",
    "        \n",
    "        # Don't track gradients for performance in evaluation\n",
    "        with torch.no_grad():\n",
    "            # Get prediction with forward pass\n",
    "#             prediction = net(image).argmax(dim=-1)\n",
    "            \n",
    "            # Get the indices of the top_n predictions\n",
    "            prediction = net(image).topk(k=top_n, dim=-1)[1]\n",
    "            \n",
    "            # Total number in batch\n",
    "            total += image.size(0)\n",
    "            \n",
    "            # Number correct in batch\n",
    "#             correct += (prediction == label).sum().item()\n",
    "            # Number correct in batch\n",
    "            correct += (prediction == tiled_labels).sum().item()\n",
    "            \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation method\n",
    "def validate(net, optim, criterion, val_loader):\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for image_cpu, label_cpu in tqdm(val_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Don't track gradients for performance in validation\n",
    "        with torch.no_grad():\n",
    "            # Forward through the network\n",
    "            output = net(image)\n",
    "            \n",
    "            # Get prediction with forward pass\n",
    "            prediction = output.argmax(dim=-1)\n",
    "\n",
    "            # Loss and gradient\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            # Total number in batch\n",
    "            total += image.size(0)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Number correct in batch\n",
    "            correct += (prediction == label).sum().item()\n",
    "            \n",
    "    return running_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create criterion\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=3, gamma=0.1)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "#                                                  mode=\"min\",\n",
    "#                                                  factor=0.2,\n",
    "#                                                  patience=1,\n",
    "#                                                  verbose=True,\n",
    "#                                                  threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "num_epochs = 12\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "best_state_dict = {}\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "#     val_acc = evaluate(model, art_val_loader) * 100\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    train(model, optimizer, criterion, art_train_loader)\n",
    "    val_loss, val_acc = validate(model, optimizer, criterion, art_val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(\"Val Loss - Epoch {}: {}\".format(epoch, val_loss))\n",
    "    print(\"Val Acc - Epoch {}: {}%\".format(epoch, val_acc))\n",
    "    \n",
    "#     if epoch % 4 == 0 and epoch != 0:\n",
    "#         torch.save(best_state_dict, \"./models/art_culture_temp_epoch\" + str(epoch))\n",
    "    \n",
    "print(\"Done! {}%\".format(evaluate(model, art_val_loader) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(best_state_dict, \"./models/art_culture_vgg16bn_steplr0001_step3_gamma01_epoch12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a5e1071ce84931ba7602cc8482fb5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=83), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! 0.918429003021148\n"
     ]
    }
   ],
   "source": [
    "# Load the model to test\n",
    "model = initialize_model(num_classes=11,\n",
    "                         resume_from=\"./models/art_culture_platlr0001_fac02_pat1_thres001_epoch30\").to(device)\n",
    "\n",
    "# model = initialize_model(num_classes=11).to(device)\n",
    "# transfer_learning()\n",
    "# model.load_state_dict(torch.load(\"./models/art_culture_transfer_epoch30\"))\n",
    "\n",
    "test_acc = evaluate(net=model, val_loader=art_test_loader, top_n=3)\n",
    "print(\"Done!\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from io import BytesIO\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_featuremap_unit(resnet,unit_id,im_input):\n",
    "    #Extract activation from model\n",
    "    #TODO: remove the last 2 layers of resnet \n",
    "    model_cut = nn.Sequential(*(list(resnet.children())[:-2]))\n",
    "\n",
    "    # Mark the model as being used for inference\n",
    "    model_cut.eval()\n",
    "    # Crop the image\n",
    "    im = transform(im_input).to(device)\n",
    "    # Place the image into a batch of size 1, and use the model to get an intermediate representation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model_cut(im.unsqueeze(0))\n",
    "        # Print the shape of our representation\n",
    "        print(out.size())\n",
    "        # Extract the only result from this batch, and take just the `unit_id`th channel\n",
    "    #     out_final = out.squeeze()[unit_id]\n",
    "\n",
    "        out_sums = out.squeeze().sum(dim=(1,2), keepdim=True)\n",
    "        out_sums = torch.where(out_sums != 0, out_sums, torch.ones(out_sums.size()).to(device))\n",
    "        out_final = (out.squeeze() / 1).sum(dim=0)\n",
    "\n",
    "    #     out_final = out.squeeze().sum(dim=0)\n",
    "\n",
    "    #     print(out_final.size())\n",
    "        print(out_final)\n",
    "\n",
    "        # Return this channel\n",
    "        return out_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_featuremap(im_input,feature_map,alpha=.4):\n",
    "    # Normalize to [0..1], with a little leeway (0.9999) in case feature_map has 0 range\n",
    "    feature_map = feature_map/(feature_map.max()+1e-10)\n",
    "    # Convert to numpy (detach() just seperates a tensor from the gradient)\n",
    "    feat_numpy = feature_map.detach().cpu().numpy()\n",
    "    # Resize the feature map to our original image size (our strided conv layers reduce the size of the image)\n",
    "    feat_numpy = cv2.resize(feat_numpy,(im_input.shape[1],im_input.shape[0]))\n",
    "    # Invert to make the heatmap look more natural\n",
    "    map_t = 1-feat_numpy\n",
    "    # Add an extra dimension to make this a [H,W,C=1] image \n",
    "    feat_numpy = np.expand_dims(feat_numpy, axis=2)\n",
    "    \n",
    "    # Convert to image (UINT8 from 0-255)\n",
    "    map_t = 255*map_t\n",
    "    map_t = map_t.astype(np.uint8)\n",
    "    # Use a color map to change this from BW to a nice color\n",
    "    map_t = cv2.applyColorMap(map_t, cv2.COLORMAP_JET)\n",
    "    # Combine the heatmap with the original image so you can see which section of the image is activated\n",
    "    im_final = np.multiply((alpha*im_input + (1-alpha)*map_t), feat_numpy) + np.multiply(im_input, 1-feat_numpy)\n",
    "    # Return final visualization\n",
    "    return im_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 7, 7])\n",
      "tensor([[ 612.1138,  488.4475,  404.6306,  237.8587,  224.0370,  296.4944,\n",
      "          393.4713],\n",
      "        [ 932.9291,  776.7737,  544.0195,  324.2351,  315.1618,  449.3172,\n",
      "          522.7351],\n",
      "        [ 944.7315,  831.6008,  559.4662,  503.8557,  501.3406,  557.8036,\n",
      "          546.6239],\n",
      "        [1080.4880, 1034.0703,  882.6573,  981.6846, 1062.5647,  971.6472,\n",
      "          722.0065],\n",
      "        [1076.6140, 1272.3844, 1340.1459, 1491.9670, 1371.8928, 1474.0098,\n",
      "         1242.7021],\n",
      "        [ 986.7207, 1314.5459, 1504.9576, 1698.7623, 1648.4434, 1728.7341,\n",
      "         1452.1600],\n",
      "        [ 759.0619, 1069.2898, 1237.6929, 1795.9281, 1826.7205, 1621.8315,\n",
      "         1403.0513]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# img0 = load_image(\"test_images/american_gothic.jpg\").to(device)\n",
    "\n",
    "culture = \"japanese\"\n",
    "image_index = \"965\"\n",
    "\n",
    "img0 = PIL.Image.open(\"test_images/las_meninas.jpg\").convert(\"RGB\")\n",
    "# img0 = PIL.Image.open(\"./data/art_culture_all_filtered/\" + culture + \"/\" + str(image_index) + \".jpg\").convert(\"RGB\")\n",
    "\n",
    "feat = generate_featuremap_unit(model,0,img0)\n",
    "\n",
    "# # Apply transforms (resize and center crop)\n",
    "img0 = img0.resize((\n",
    "    int(256*img0.width/min(img0.width, img0.height)),\n",
    "    int(256*img0.height/min(img0.width, img0.height)),\n",
    "))\n",
    "img0 = img0.crop((\n",
    "    (img0.width - 224)/2,\n",
    "    (img0.height - 224)/2,\n",
    "    (img0.width + 224)/2,\n",
    "    (img0.height + 224)/2,\n",
    "))\n",
    "img_numpy = np.array(img0)\n",
    "\n",
    "im_final = visualize_featuremap(img_numpy,feat)\n",
    "showarray(im_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_culture_center(culture, model):\n",
    "    '''Calculates the average Euclidean center of the all image outputs for a culture using model'''\n",
    "    \n",
    "    # Get list of all image files in the appropriate folder\n",
    "    image_files = os.listdir(\"data/art_culture_all_filtered/\" + culture)[:]\n",
    "    \n",
    "    # Switch to evaulation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Variable for accumulating outputs\n",
    "    total_tensor = torch.zeros((1,11)).to(device)\n",
    "    total_images = len(image_files)\n",
    "    \n",
    "    # Iteratively accumulate outputs in total_tensor\n",
    "    for file in image_files:\n",
    "        image = load_image(\"data/art_culture_all_filtered/\" + culture + \"/\" + file).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            result_vec = model(image)\n",
    "            total_tensor += result_vec\n",
    "            \n",
    "    return total_tensor / total_images\n",
    "\n",
    "# print(compute_culture_center(\"spanish\", model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Euclidean center of all image outputs for each culture using best model\n",
    "culture_tensors = {\n",
    "    \"american\" : torch.tensor([[ 7.0664,  1.6778, -1.8836, -0.8605, -1.1828,  0.9407, -0.0519, -1.8947, -1.8577, -0.9771, -0.6822]]),\n",
    "    \"british\" : torch.tensor([[ 1.3715e+00,  3.6504e+00, -2.1580e+00, -2.0042e-03,  3.3697e-01, 2.0065e+00,  9.8763e-01, -2.6181e+00, -1.0272e+00, -2.6346e+00, -2.5038e-01]]),\n",
    "    \"chinese\" : torch.tensor([[-0.6801, -1.5646,  7.2309, -1.5411, -1.5731, -0.1273, -1.0995,  0.2166, -1.4205,  2.1701, -0.9345]]),\n",
    "    \"dutch\" : torch.tensor([[-0.7726,  0.0573, -2.4268,  3.7708,  0.8802,  1.4279,  1.2017, -3.0107, 1.2312, -3.2050,  0.4194]]),\n",
    "    \"flemish\" : torch.tensor([[-0.5941,  0.9288, -2.2716,  1.5399,  1.8579,  1.8715,  0.8328, -2.9006, 1.2896, -2.9780,  0.3761]]),\n",
    "    \"french\" : torch.tensor([[ 0.2759,  0.8285, -2.0896,  0.7542,  0.6249,  3.9568,  0.4689, -3.0201, 0.2730, -2.1955, -0.0474]]),\n",
    "    \"german\" : torch.tensor([[-0.1850,  0.7377, -2.0441,  1.2382,  0.3095,  1.1902,  1.7258, -2.0748, 1.1933, -2.6024,  0.3569]]),\n",
    "    \"indian\" : torch.tensor([[-0.6212, -2.5173,  1.3177, -2.3141, -2.8249, -1.3583, -0.5100,  7.6875, -0.5083,  0.6289,  0.6106]]),\n",
    "    \"italian\" : torch.tensor([[-1.8449, -1.1914, -2.2632,  0.9813,  0.5290,  0.8880,  1.4288, -1.7552, 5.1137, -2.9153,  0.9664]]),\n",
    "    \"japanese\" : torch.tensor([[ 0.1553, -2.1064,  2.4003, -2.0872, -2.0149, -0.1749, -1.4981,  0.2818, -1.4746,  8.0753, -0.9232]]),\n",
    "    \"spanish\" : torch.tensor([[-0.6073, -0.2381, -1.3960,  0.5750,  0.0869,  1.0328,  0.6943, -0.9015, 0.9975, -2.0954,  1.6449]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_between_cultures(culture1, culture2):\n",
    "    '''Computes the Euclidean distance between two culture centers'''\n",
    "    return torch.dist(culture_tensors[culture1], culture_tensors[culture2]).item()\n",
    "\n",
    "def compute_all_distances_from_culture(culture1):\n",
    "    '''Computes the Eucliean distance of all cultures from the specified culture'''\n",
    "    results = []\n",
    "    for culture2 in culture_categories_list:\n",
    "        results.append((culture2, compute_distance_between_cultures(culture1, culture2)))\n",
    "    \n",
    "    return [x[1] for x in results]\n",
    "    \n",
    "#     return sorted(results, key=lambda x: x[1])\n",
    "\n",
    "# for pair in compute_all_distances_from_culture(\"american\"):\n",
    "#     print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "culture_distances = []\n",
    "for culture in culture_categories_list:\n",
    "    culture_distances.append(compute_all_distances_from_culture(culture))\n",
    "    \n",
    "for row in range(len(culture_distances)):\n",
    "    # Apply function to each value\n",
    "    for col in range(len(culture_distances[row])):\n",
    "        if not (abs(culture_distances[row][col]) <= 0.00000001):\n",
    "            culture_distances[row][col] = 1/culture_distances[row][col]**2\n",
    "            \n",
    "#     Normalize each row/culture\n",
    "    row_sum = sum(culture_distances[row])\n",
    "    for col in range(len(culture_distances[row])):\n",
    "        if not (abs(culture_distances[row][col]) <= 0.00000001):\n",
    "            culture_distances[row][col] = culture_distances[row][col]/row_sum\n",
    "    \n",
    "# print(culture_distances)\n",
    "\n",
    "print(\"labels\" + \" \" + \" \".join(culture_categories_list))\n",
    "for row in range(len(culture_distances)):\n",
    "#     print(culture_distances[row])\n",
    "    print(culture_categories_list[row] + \" \" + \" \".join([str(x*100000) for x in culture_distances[row]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#ff0000\", \"#ff8c00\", \"#eaff00\", \"#5eff00\", \"#00ff2f\", \"#00ffbb\", \"#00bbff\", \"#002fff\", \"#5d00ff\", \"#ea00ff\", \"#ff0088\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "culture_categories = {\n",
    "    0 : \"american\",\n",
    "    1 : \"british\",\n",
    "    2 : \"chinese\",\n",
    "    3 : \"dutch\",\n",
    "    4 : \"flemish\",\n",
    "    5 : \"french\",\n",
    "    6 : \"german\",\n",
    "    7 : \"indian\",\n",
    "    8 : \"italian\",\n",
    "    9 : \"japanese\",\n",
    "    10 : \"spanish\"\n",
    "}\n",
    "\n",
    "culture_categories_list = [\"american\",\n",
    "                           \"british\",\n",
    "                           \"chinese\",\n",
    "                           \"dutch\",\n",
    "                           \"flemish\",\n",
    "                           \"french\",\n",
    "                           \"german\",\n",
    "                           \"indian\",\n",
    "                           \"italian\",\n",
    "                           \"japanese\",\n",
    "                           \"spanish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.7958e-01, 8.0352e-02, 1.7556e-02, 8.8009e-03, 3.7669e-03, 2.4896e-03,\n",
      "         2.2478e-03, 1.9698e-03, 1.7611e-03, 1.4066e-03, 6.8982e-05]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "[('american',), ('japanese',), ('french',), ('italian',), ('german',), ('british',), ('spanish',), ('dutch',), ('indian',), ('flemish',), ('chinese',)]\n"
     ]
    }
   ],
   "source": [
    "# Test on one specific example\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_image(image_name):\n",
    "    \"\"\"Loads an image; returns a batched image tensor\"\"\"\n",
    "    image = PIL.Image.open(image_name).convert(\"RGB\")\n",
    "#     plt.imshow(image)\n",
    "    \n",
    "    image = transform(image)\n",
    "    \n",
    "#     plt.imshow(image.permute(1,2,0))\n",
    "    \n",
    "    image = image.clone().detach().requires_grad_(True)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def classify_image(model, categories, image_name):\n",
    "    '''Classify an image using a specified model'''\n",
    "    model.eval()\n",
    "    image = load_image(image_name).to(device)\n",
    "    result_vec = model(image)\n",
    "    \n",
    "    sorted_result_vec, sorted_indices = result_vec.sort(descending=True)\n",
    "    \n",
    "    results = sorted_indices[0].tolist()\n",
    "    probabilities = torch.nn.functional.softmax(sorted_result_vec)\n",
    "    print(probabilities)\n",
    "    \n",
    "    return [(categories[index],) for index in results]\n",
    "\n",
    "model = initialize_model(num_classes=11,\n",
    "                         resume_from=\"./models/art_culture_platlr0001_fac02_pat1_thres001_epoch30\").to(device)\n",
    "\n",
    "culture = \"spanish\"\n",
    "image_index = \"27\"\n",
    "\n",
    "print(classify_image(model,\n",
    "                     culture_categories,\n",
    "                     \"test_images/memory.jpg\"))\n",
    "\n",
    "# print(classify_image(model,\n",
    "#                      culture_categories,\n",
    "#                      \"./data/art_culture_test/\" + culture + \"/\" + str(image_index) + \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing data from directory\n",
    "letters_val = torchvision.datasets.ImageFolder(root=\"./data/text_val\",\n",
    "                                               transform=transform)\n",
    "\n",
    "letters_val_loader = torch.utils.data.DataLoader(dataset=letters_val,\n",
    "                                                 batch_size=512,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "# Initialize model\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "# model = torchvision.models.resnet50(pretrained=False)\n",
    "\n",
    "# Set number of output classes\n",
    "model.conv1 = nn.Conv2d(in_channels=3,\n",
    "                        out_channels=64,\n",
    "                        kernel_size=(7,7),\n",
    "                        stride=(2,2),\n",
    "                        padding=(3,3),\n",
    "                        bias=False)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "out_features = 26\n",
    "model.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"./models/letter_model_lr01_gamma015_e12\"))\n",
    "\n",
    "val_acc = evaluate(model, letters_val_loader) * 100\n",
    "print(\"Done!\", val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
