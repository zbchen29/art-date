{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Art Dating\n",
    "\n",
    "#### Students\n",
    "- Zhenbang Chen\n",
    "- Zhenjia Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Importing packages and dependencies.  Load dataset for categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "rootpath = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\zhenbang\\anaconda3\\lib\\site-packages (4.39.0)\n",
      "PyTorch Version:  1.3.1\n",
      "Torchvision Version:  0.4.2\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# You might not have tqdm, which gives you nice progress bars\n",
    "!pip install tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, resume_from=None):\n",
    "    \n",
    "    # Model (nn.Module) to return\n",
    "    # model_ft = models.resnet18(pretrained = false)\n",
    "    model_ft = models.resnet50(pretrained = true)\n",
    "    \n",
    "    in_features = 512\n",
    "    model_ft.fc = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to apply to the data\n",
    "# transform = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "#     torchvision.transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "# ])\n",
    "\n",
    "# Transform to apply to the data for use with pretrained ResNet model\n",
    "transform = torchvision.transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data from directory\n",
    "art_train = torchvision.datasets.ImageFolder(root=\"./data/art_train\",\n",
    "                                                 transform=transform)\n",
    "\n",
    "# Get testing data from directory\n",
    "art_val = torchvision.datasets.ImageFolder(root=\"./data/art_val\",\n",
    "                                               transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random sampler\n",
    "random_sampler = torch.utils.data.RandomSampler(data_source=art_train,\n",
    "                                                replacement=True,\n",
    "                                                num_samples=int(len(art_train)/40))\n",
    "\n",
    "# Create batched dataloader\n",
    "# art_train_loader = torch.utils.data.DataLoader(dataset=art_train,\n",
    "#                                                    batch_size=8,\n",
    "#                                                    shuffle=True,\n",
    "#                                                    num_workers=4,\n",
    "#                                                    pin_memory=True)\n",
    "\n",
    "art_train_loader = torch.utils.data.DataLoader(dataset=art_train,\n",
    "                                                   batch_size=8,\n",
    "                                                   sampler=random_sampler,\n",
    "                                                   shuffle=False,\n",
    "                                                   num_workers=4,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "art_val_loader = torch.utils.data.DataLoader(dataset=art_val,\n",
    "                                                 batch_size=8,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to C:\\Users\\Zhenbang/.cache\\torch\\checkpoints\\resnet50-19c8e357.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:22<00:00, 4.60MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "# model = torchvision.models.resnet18(pretrained=False)\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Set number of output classes\n",
    "# model.conv1 = nn.Conv2d(in_channels=3,\n",
    "#                         out_channels=64,\n",
    "#                         kernel_size=(7,7),\n",
    "#                         stride=(2,2),\n",
    "#                         padding=(3,3),\n",
    "#                         bias=False)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "out_features = 20\n",
    "model.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training method\n",
    "def train(net, optim, train_loader):\n",
    "    net.train()\n",
    "    for image_cpu, label_cpu in tqdm(train_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        \n",
    "        # Clear gradient\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # Forward through the network\n",
    "        output = net(image)\n",
    "        \n",
    "        # Loss and gradient\n",
    "        loss = torch.nn.functional.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update paramters\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation method\n",
    "def evaluate(net, val_loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for image_cpu, label_cpu in tqdm(val_loader):\n",
    "        # Move image and label to GPU\n",
    "        image = image_cpu.to(device)\n",
    "        label = label_cpu.to(device)\n",
    "        \n",
    "        # Don't track gradients for performance in evaluation\n",
    "        with torch.no_grad():\n",
    "            # Get prediction with forward pass\n",
    "            prediction = net(image).argmax(dim=-1)\n",
    "            \n",
    "            # Total number in batch\n",
    "            total += image.size(0)\n",
    "            \n",
    "            # Number correct in batch\n",
    "            correct += (prediction == label).sum().item()\n",
    "            \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=4, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a08e802be9f4a16bed517bc91d39b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: 0.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24a3e42b42d432fb865ec386a03eb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f3aa323bb74cae9ca3f987754baa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: 49.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccba8ae7a9f4d51b807ff863ccd5447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9a6b4e0ad747d18083a32f1eb18499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: 0.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f2783f81264247b8e4256a0e2a2c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 138, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 174, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 156, in pil_loader\n    img = Image.open(f)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 2818, in open\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\nOSError: cannot identify image file <_io.BufferedReader name='./data/art_train\\\\17\\\\1194.jpg'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-139ab906888f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch {}: {}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mart_train_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f35337f66ce1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, optim, train_loader)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimage_cpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_cpu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Move image and label to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_cpu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1091\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 138, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 174, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 156, in pil_loader\n    img = Image.open(f)\n  File \"C:\\Users\\Zhenbang\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 2818, in open\n    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\nOSError: cannot identify image file <_io.BufferedReader name='./data/art_train\\\\17\\\\1194.jpg'>\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "num_epochs = 4\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "best_state_dict = {}\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    val_acc = evaluate(model, art_val_loader) * 100\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    print(\"Epoch {}: {}%\".format(epoch, val_acc))\n",
    "    train(model, optimizer, art_train_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % 4 == 0 and epoch != 0:\n",
    "        torch.save(best_state_dict, \"./models/art_dept6_date_lr01\" + str(epoch))\n",
    "    \n",
    "print(\"Done! {}%\".format(evaluate(model, art_val_loader) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(best_state_dict, \"./models/letter_shifts_class732_lr01_epoch12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get testing data from directory\n",
    "letters_val = torchvision.datasets.ImageFolder(root=\"./data/text_val\",\n",
    "                                               transform=transform)\n",
    "\n",
    "\n",
    "letters_val_loader = torch.utils.data.DataLoader(dataset=letters_val,\n",
    "                                                 batch_size=512,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=4,\n",
    "                                                 pin_memory=True)\n",
    "\n",
    "# Initialize model\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "# model = torchvision.models.resnet50(pretrained=False)\n",
    "\n",
    "# Set number of output classes\n",
    "model.conv1 = nn.Conv2d(in_channels=3,\n",
    "                        out_channels=64,\n",
    "                        kernel_size=(7,7),\n",
    "                        stride=(2,2),\n",
    "                        padding=(3,3),\n",
    "                        bias=False)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "out_features = 26\n",
    "model.fc = nn.Linear(in_features, out_features)\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"./models/letter_model_lr01_gamma015_e12\"))\n",
    "\n",
    "val_acc = evaluate(model, letters_val_loader) * 100\n",
    "print(\"Done!\", val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
